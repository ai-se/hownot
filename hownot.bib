@article{Menzies2010,
abstract = {Building quality software is expensive and software quality assurance (QA) budgets are limited. Data miners can learn defect predictors from static code features which can be used to control QA resources; e.g. to focus on the parts of the code predicted to be more defective. Recent results show that better data mining technology is not leading to better defect predictors. We hypothesize that we have reached the limits of the standard learning goal of maximizing area under the curve (AUC) of the probability of false alarms and probability of detection AUC(pd, pf) ; i.e. the area under the curve of a probability of false alarm versus probability of detection. Accordingly, we explore changing the standard goal. Learners that maximize AUC(effort, pd) find the smallest set of modules that contain the most errors. WHICH is a meta-learner framework that can be quickly customized to different goals. When customized to AUC(effort, pd), WHICH out-performs all the data mining methods studied here. More importantly, measured in terms of this new goal, certain widely used learners perform much worse than simple manual methods. Hence, we advise against the indiscriminate use of learners. Learners must be chosen and customized to the goal at hand. With the right architecture (e.g. WHICH), tuning a learner to specific local business goals can be a simple task.},
author = {Menzies, Tim and Milton, Zach and Turhan, Burak and Cukic, Bojan and Jiang, Yue and Bener, Ay≈üe},
doi = {10.1007/s10515-010-0069-5},
issn = {09288910},
journal = {Automated Software Engineering},
keywords = {Defect prediction,Static code features,Which},
number = {4},
pages = {375--407},
title = {{Defect prediction from static code features: Current results, limitations, new approaches}},
volume = {17},
year = {2010}
}



@article{Irani1993,
abstract = {Since most real-world applications of classification learning involve continuous-valued attributes, properly addressing the discretization process is an important problem. This paper addresses the use of the entropy minimization heuristic for discrctizing the range of a continuous-valued attribute into multiple intervals. We briefly present theoretical evidence for the apropiateness of this heuristic for use in the binary discretization algorithm used in ID3, C4, CART, and other learning algorithms. The results serve to justify extending the algorithm to derive multiple intervals. We formally derive a criterion based on the minimum description length principle for deciding the partition of intervals. We demonstrate via empirical evaluation on several real-world data sets that better decision trees are obtained using the new multi-interval algorithm.},
author = {Irani, KB and Fayyad, UM},
file = {::},
isbn = {1-55860-300-X},
issn = {15730565},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
pages = {1022--1027},
title = {{Multi-lnterval Discretization of Continuous-Valued Attributes for Classification learning}},
year = {1993}
}


@article{Anda2009,
abstract = {The scientific study of a phenomenon requires it to be reproducible. Mature engineering industries are recognized by projects and products that are, to some extent, reproducible. Yet, reproducibility in software engineering (SE) has not been investigated thoroughly, despite the fact that lack of reproducibility has both practical and scientific consequences. We report a longitudinal multiple-case study of variations and reproducibility in software development, from bidding to deployment, on the basis of the same requirement specification. In a call for tender to 81 companies, 35 responded. Four of them developed the system independently. The firm price, planned schedule, and planned development process, had, respectively, ldquolow,rdquo ldquolow,rdquo and ldquomediumrdquo reproducibilities. The contractor's costs, actual lead time, and schedule overrun of the projects had, respectively, ldquomedium,rdquo ldquohigh,rdquo and ldquolowrdquo reproducibilities. The quality dimensions of the delivered products, reliability, usability, and maintainability had, respectively, ldquolow,rdquo "high,rdquo and ldquolowrdquo reproducibilities. Moreover, variability for predictable reasons is also included in the notion of reproducibility. We found that the observed outcome of the four development projects matched our expectations, which were formulated partially on the basis of SE folklore. Nevertheless, achieving more reproducibility in SE remains a great challenge for SE research, education, and industry.},
author = {Anda, Bente C D and Sj\o berg, Dag I K and Mockus, Audris},
doi = {10.1109/TSE.2008.89},
isbn = {0098-5589},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Multiple-case study,Software engineering life cycle,Software process,Software project success,Software quality},
number = {3},
pages = {407--429},
title = {{Variability and reproducibility in software engineering: A study of four companies that developed the same system}},
volume = {35},
year = {2009}
}


@article{Menzies2013,
abstract = {Existing research is unclear on how to generate lessons learned for defect prediction and effort estimation. Should we seek lessons that are global to multiple projects or just local to particular projects? This paper aims to comparatively evaluate local versus global lessons learned for effort estimation and defect prediction. We applied automated clustering tools to effort and defect datasets from the PROMISE repository. Rule learners generated lessons learned from all the data, from local projects, or just from each cluster. The results indicate that the lessons learned after combining small parts of different data sources (i.e., the clusters) were superior to either generalizations formed over all the data or local lessons formed from particular projects. We conclude that when researchers attempt to draw lessons from some historical data source, they should 1) ignore any existing local divisions into multiple sources, 2) cluster across all available data, then 3) restrict the learning of lessons to the clusters from other sources that are nearest to the test data.},
author = {Menzies, Tim and Butcher, Andrew and Cok, David and Marcus, Andrian and Layman, Lucas and Shull, Forrest and Turhan, Burak and Zimmermann, Thomas},
doi = {10.1109/TSE.2012.83},
issn = {00985589},
journal = {IEEE Transactions on Software Engineering},
keywords = {Data mining,clustering,defect prediction,effort estimation},
number = {6},
pages = {822--834},
title = {{Local versus global lessons for defect prediction and effort estimation}},
volume = {39},
year = {2013}
}


@mastersthesis{div14,
  title="Exploring Essential content of Defect Prediction 
         and Effort Estimation through Data Reduction",
  year=2014,
  author="Divya Ganesan",
  school="Computer Science, West Virginia University"
 }
 
 
@mastersthesis{nva14,
  title="Cross Trees: Visualizing Estimations using Decision Trees",
  year=2014,
  author="Naveen  Lekkalapudi",
  school="Computer Science, West Virginia University"
 }

@book{fastmap,
  title={FastMap: A fast algorithm for indexing, data-mining and visualization of traditional and multimedia datasets},
  author={Faloutsos, Christos and Lin, King-Ip},
  volume={24},
  number={2},
  year={1995},
  publisher={ACM}
}
@ARTICLE{6600685, 
author={Menzies, T. and Brady, A. and Keung, J. and Hihn, J. and Williams, S. and El-Rawas, O. and Green, P. and Boehm, B.}, 
journal={Software Engineering, IEEE Transactions on}, 
title={Learning Project Management Decisions: A Case Study with Case-Based Reasoning versus Data Farming}, 
year={2013}, 
month={Dec}, 
volume={39}, 
number={12}, 
pages={1698-1713}, 
keywords={Monte Carlo methods;case-based reasoning;data handling;learning (artificial intelligence);project management;sampling methods;software management;CBR;Monte Carlo sampling;SEESAW;case-based reasoning;data farming;project management decision learning;software project data;Data models;Mathematical model;Monte Carlo methods;Project management;Search methods;Software engineering;COCOMO;Search-based software engineering;case-based reasoning;data farming}, 
doi={10.1109/TSE.2013.43}, 
ISSN={0098-5589},}
@inproceedings{kocaguneli2010use,
  title={When to use data from other projects for effort estimation},
  author={Kocaguneli, Ekrem and Gay, Gregory and Menzies, Tim and Yang, Ye and Keung, Jacky W},
  booktitle={Proceedings of the IEEE/ACM international conference on Automated software engineering},
  pages={321--324},
  year={2010},
  organization={ACM}
}
@article{HOW,
author = {Krishna, Rahul and Menzies, Tim and Shen, Xipeng and Marcus, Andrian},
file = {:Users/rkrsn/Documents/Mendeley Desktop/Krishna et al. - Learning Actionable Analytics ( with applications for reducing defects and reducing runtimes ).pdf:pdf},
title = {{Learning Actionable Analytics ( with applications for reducing defects and reducing runtimes )}}
}

@inproceedings{jureczko10,
author = {Jureczko, Marian and Madeyski, Lech},
booktitle = {Proceedings of the 6th International Conference on Predictive Models in Software Engineering},
pages = {9:1----9:10},
publisher = {ACM},
series = {PROMISE '10},
title = {{Towards identifying software project clusters with regard to defect prediction}},
year = {2010}
}
@article{mittas13,
  author =	{Nikolaos Mittas and Lefteris Angelis},
  title =	{Ranking and Clustering Software Cost Estimation
                  Models through a Multiple Comparisons Algorithm},
  journal =	{IEEE Trans. Software Eng.},
  volume =	39,
  number =	4,
  year =	2013,
  pages =	{537-551},
}