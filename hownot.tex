\documentclass[conference]{IEEEtran}
%future work: local patch operator
%complexity

\usepackage{subfig}
\usepackage{wrapfig}
 \usepackage{amsmath}
 \usepackage{url}
 \usepackage{pifont}
 %\usepackage{times}
\usepackage{rotating}
%\usepackage{balance} 
\usepackage{color, colortbl}
\usepackage{graphicx}
\usepackage{algorithmicx}
\usepackage[running]{lineno}
\usepackage{program}
\usepackage{cite}
\usepackage{alltt}
\usepackage{balance}
\newcommand{\eq}[1]{Equation~\ref{eq:#1}}
\newcommand{\bi}{\begin{itemize}}
\newcommand{\ei}{\end{itemize}}
\newcommand{\be}{\begin{enumerate}}
\newcommand{\ee}{\end{enumerate}}
\newcommand{\tion}[1]{\textsection\ref{sec:#1}}
\newcommand{\fig}[1]{Figure~\ref{fig:#1}}
\definecolor{lightgray}{gray}{0.975}
\usepackage{fancyvrb}
\usepackage{stfloats}
\usepackage{multirow}
\usepackage{listings}
\usepackage{amsmath}  
\DeclareMathOperator*{\argmin}{arg\,min} 
\DeclareMathOperator*{\argmax}{arg\,max} 
%\usepackage[usenames]{xcolor}




\usepackage{color}
\newcommand{\colorrule}[1]{\begingroup\color{#1}\hrule\endgroup}

\definecolor{darkgreen}{rgb}{0,0.3,0}

\usepackage[table]{xcolor}
\definecolor{Gray}{rgb}{0.88,1,1}
\definecolor{Gray}{gray}{0.85}
\definecolor{Blue}{RGB}{0,29,193}
\newcommand{\G}{\cellcolor{green}}
\newcommand{\Y}{\cellcolor{yellow}}


\definecolor{MyDarkBlue}{rgb}{0,0.08,0.45} 
\newenvironment{changed}{\par\color{MyDarkBlue}}{\par}

\newcommand{\ADD}[1]{\textcolor{MyDarkBlue}{{\bf #1}}}
\newcommand{\addit}[1]{\begin{changed}\input{#1}\end{changed}}

\usepackage{color}
\usepackage{listings}
\usepackage{setspace}

\definecolor{Gray}{gray}{0.9}
\newcommand{\kw}[1]{\textit{#1}}
\newcommand{\quart}[4]{\begin{picture}(75,6)
  {\color{black}\put(#3,3){\circle*{2.5}}\put(#1,3){\line(1,0){#2}}}\end{picture}}
% New Commands

\definecolor{Code}{rgb}{0,0,0}
\definecolor{Decorators}{rgb}{0.5,0.5,0.5}
\definecolor{Numbers}{rgb}{0.5,0,0}
\definecolor{MatchingBrackets}{rgb}{0.25,0.5,0.5}
\definecolor{Keywords}{rgb}{0,0,1}
\definecolor{self}{rgb}{0,0,0}
\definecolor{Strings}{rgb}{0,0.63,0}
\definecolor{Comments}{rgb}{0,0.63,1}
\definecolor{Comments}{rgb}{0.5,0.5,0.5}
\definecolor{Backquotes}{rgb}{0,0,0}
\definecolor{Classname}{rgb}{0,0,0}
\definecolor{FunctionName}{rgb}{0,0,0}
\definecolor{Operators}{rgb}{0,0,0}
\definecolor{Background}{rgb}{1,1,1}
\title{"Peeking" for Actionable Analytics}

% You can go ahead and credit any number of authors here,
% e.g. one 'row of three' or two rows (consisting of one row of three
% and a second row of one, two or three).
%
% The command \alignauthor (no curly braces needed) should
% precede each author name, affiliation/snail-mail address and
% e-mail address. Additionally, tag each Slope of
% affiliation/address with \affaddr, and tag the
% e-mail address with \email.
%
% 1st. author
\author{Rahul Krishna, Tim Menzies\\
        Computer Science, North Carolina State University, USA\\
       \{i.m.ralk, tim.menzies\}\@gmail.com
       
       % use '\and' if you need 'another row' of author names
% 4th. author
}


  \pagestyle{plain}
\begin{document}
  \maketitle
    \begin{abstract}
There are many
    algorithms for data {\em classification} such as  C4.5, Naive Bayes, SVM, etc. While these algorithms are widely developed and well-known, another kind
    of reasoning has far less support.
    
This paper is about that other kind of reasoning. Here, we seek not
    to recognize ``what is'', but instead to offer advice on ``what to change''.
Two approaches for learning minimal, yet effective changes, to software
artifacts are explored. Lessons learned are reported.
  \end{abstract}
  \begin{IEEEkeywords}
Prediction, case-based reasoning, data mining, software engineering.
  \end{IEEEkeywords}

\section{Introduction} 
How should we handle ``unpopulular'' results
    for data analytics? For example, if a business manager is presented
    with a data mining result that troubles them (e.g. an estimate of
    development time that is distressingly long), what advice
    can we offer them on management actions to {\em change} hat estimate?
    
This is an important question, and one with much currency in industry. 
In the summer of 2011 and 2012, one of us (Menzies) spent two months
working on-site at Microsoft Redmond, 
observing data mining analysts.  He
noted how Microsoft's data scientists
discussed their data with  business users. In summary, much
of that discussion was {\em not} supported by standard data miners.

One surprising observation made at Microsoft  was that
there was a surprising little
inspection of the output of data miners as compared to another process, which we call {\em peeking}.
In {\em peeking}, analysts and users spend much time 
inspecting and discuss small samples of either raw or exemplary or synthesized project data.  Further, very little of those discussions were  focused on classification
(the additional of a labels to some unlabelled data). Rather, much time
was spent in those meetings discussion {\em what to do do}; i.e. trying
to determine what could be altered to better improve some business outcome.

That observational study of current practices
at Microsoft found two common ``peeking'' methods.
In {\em data engagement meetings},
users debated the implications of data 
displayed on a screen. In this way, users
engaged with the data and with each other by
monitoring each others' queries and check each others'
conclusions.  

Another data analysis pattern observed
at Microsoft was  {\em cluster + contrast} in which
data is  reduced to a few
  clusters. Users are then just shown the delta between those
  clusters. While contrasting, if feature values are
  the same in both clusters, then these were pruned from
  the reports to the user. In this way, very large
data sets can be shown on one PowerPoint 
slide. Note that {\em cluster+contrast} was a tool that can be usefully employed within
{\em data engagement meetings}.


Cluster+contrast and engagement
meetings are common practices at Microsoft. Yet they were
are supported by a complex and kludged combination of several
tools. Also, those methods had never been rigorously studied or certified.
For both those reasons,
 we reflected over those tools to discover and analyze their
underlying process. The result was HOW: a tool
that combines two 
data reduction methods:\bi
\item Method1: feature selection
\item Method2: centroid generation from   clusters.
\ei
While Method1 is widely used~\cite{menziesLearnToPredict},
to the best of our knowledge, this combination has not been thoroughly explored before.
Hence, this paper discusses alternative method for ``peeking''.


\section{Contrast Trees}
Contrast trees are decision trees that can be used to learn decision rules from a large data set. The use of decision trees for this purpose makes it easy to visualize the data and also makes interpretation of the recommended policies fairly straight forward. In addition to this, contrast tree generate ranges of decisions that allow for better feasibility.
\subsection{How does a contrast tree work?}
Contrast trees works by building a simple decision tree based on entropy, as in the classification and regression trees. Contrast tree uses the WHERE clustering algorithm, which in-turn uses FASTMAP \cite{} to compute clusters 
that contain instances that are spatially close. Each cluster is assigned a unique cluster id. Then, a simple decision tree is built using a standard classification and regression tree (CART). 

The leaves of the tree contain several instances usually from more than one cluster. The leaves are scored using the mean of the majority cluster. The construction of the tree is subject to the following tunable features:
\begin{itemize}
\item \textit{Entropy} is used as criteria for generating optimum cuts while building the decision trees. These were shown to be consistent in giving high performance across most data.
\item \textit{Minimum samples per leaf} defines the minimum number of instances in a leaf of the decision tree.
\item \textit{Max depth} defines the number of levels of recursive splits that are allowed in the tree.
\end{itemize}

Decision trees can be used to generate a decision rule, which when applied to solution sets must theoretically improve performance. To generate rules -- a tree is constructed with some training data, and for every instance in a the test data, we find a branch in the tree that best matches the test data. All instances that needs improvement in the test data constitute the ``worst'' set W. We find the nearest branch, ``B'', in the tree with better performance. The difference between W and B constitutes the decision rules.

A contrast set, ``C'' is simply a collection of all the decision rules. It can be obtained traversing through the tree from ``W'' to ``B'' and tracking all the branch variables along this path. Note that the branch variables use decision ranges instead of single point solutions, making implementation of the rules in real life feasible. 

In theory, contrast set is an ideal tool for generating decision rules. Some of the key advantages are listed below:
\begin{itemize}
\item They allow the user to exert a fine grain control over the parameters that needs to be changed, while providing a way to visualize the recommended changes.
\item The changes suggested by the contrast sets are inherently local in nature, making the changes practical and potentially easy to implement.
\item The contrast tress require a worst case time complexity of $O(n)$, which is a function in linear time.
\end{itemize}
\subsection{Reasons for failure}
Although the idea seemed ideal, initial implementation of the contrast tree provided results that we most discouraging. One of the major problems with using contrast trees was that the trees were usually rather large. The initial motivation was that the trees could serve as a medium for experts to identify and explore solution spaced that were local to the problem. The size of the decision tree jeopardized the readability of the solutions by increasing the complexity. In our efforts to reduce the size of the tree, we devised a pruning method, that prunes away irrelevant branches that do not contribute better solutions. The smaller trees, while being easier to understand, posed another problem: the test instances were too dissimilar to the leaves in the tree. This meant that the relevance of the contrast set could not be justified.

The depth of the tree also influences the performance of contrast sets. The depth is represented by levels of splits in the tree, and this parameter determines the number of features that require changes. If the tree is too shallow, the changes suggested by the planner are too little to make any difference. On the other hand, deeper trees leads to over-fitting and therefore suggests too many changes when not required.

A study was conducted on numerous examples of the Jureczko object-oriented static code data sets \cite{}: Ant, Camel, Ivy, Jedit, Log4j, Lucene, Poi, Synapse, Velocity, Xalan, Xerces \footnote{Available from the object-oriented defects section of the PROMISE respository openscience.us/repo/defect/ck.}. On these data sets, the plans generated by contrast tree advise
how to change static code attributes in order to reduce defects in Java classes. The challenges with contrast trees had a profound impact on the performance, as shown in figure \ref{}.
\subsection{What worked?}
Contrast tree is the second generation of our efforts to use contrast set learners for planning case based reasoning. W2 represents the first generation of our work. W2 is a CBR planner that reflected over the delta of raw attributes \cite{}. However, it frequently suffered from an optimization failure. When its plans were applied, performance improved in only  $\tfrac{1}{3}$rd of test cases. 

Generation two of our work resulted in the development of contrast trees. It uses a recursive clustering method discussed above followed by summarizing these recursive divisions into a tree structure. Although the initial results with contrast tree were weak, they were somewhat positive. We tried to extend and improve the contrast tree prototype, however the results were not encouraging. The tree-based approach suffered from one key issue: the local changes were highly ineffective in reducing the defects, see figure \ref{}. In addition, it also faced the optimization failure problem, which was also seen with W2. 

As a continuation of the contrast tree work, we built HOW as a simpler approach that was to provide a baseline result, above which contrast tree was meant to do better. However, HOW’s results were so good that we threw away months of work on tree-based planning with contrast tree. 
\section{Future Work}

\end{document}